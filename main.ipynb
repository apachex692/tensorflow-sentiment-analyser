{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Sentiment Analysis\n",
        "\n",
        "- **Author:** Sakthi Santhosh\n",
        "- **Created on:** 04/02/2023\n",
        "\n",
        "## To-Do\n",
        "\n",
        "- Integerate ```TextVectorization``` layer into the model."
      ],
      "metadata": {
        "id": "9QwX4jAB58le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Modules"
      ],
      "metadata": {
        "id": "Md1pDIkTez6i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4xID7Ba53XA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from matplotlib import font_manager, pyplot, rcParams\n",
        "from numpy import expand_dims\n",
        "import pandas\n",
        "\n",
        "from tensorflow.data import Dataset\n",
        "from tensorflow.keras.layers import (\n",
        "    Bidirectional,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    LSTM,\n",
        "    TextVectorization\n",
        ")\n",
        "from tensorflow.keras.metrics import (\n",
        "    Precision,\n",
        "    Recall,\n",
        "    CategoricalAccuracy\n",
        ")\n",
        "from tensorflow.keras.models import load_model, Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Declarations"
      ],
      "metadata": {
        "id": "ObYT8fIGgF0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 300000\n",
        "DATASET_LOCATION = \"/content/gdrive/MyDrive/Sharing/Programming/python/ai/sentiment_analysis/\""
      ],
      "metadata": {
        "id": "jI5IZ4rvgJis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Datasets"
      ],
      "metadata": {
        "id": "VMimJ-GAmPLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/gdrive/\", force_remount=True)"
      ],
      "metadata": {
        "id": "uBCNLgWgmRJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load CSV File to Pandas Dataframe"
      ],
      "metadata": {
        "id": "yyHrPuswjKG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataframe = pandas.read_csv(DATASET_LOCATION + \"sentiment_analyser_dataset\")"
      ],
      "metadata": {
        "id": "87FMsMaKjP1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the Data"
      ],
      "metadata": {
        "id": "y2DHs6x2khHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = {\n",
        "    \"comments\": training_dataframe[\"comment_text\"],\n",
        "    \"classifications\": training_dataframe[\n",
        "        training_dataframe.columns[2:]\n",
        "    ].values\n",
        "}\n",
        "\n",
        "vectorizer = TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_sequence_length=1500,\n",
        "    output_mode=\"int\"\n",
        ")\n",
        "vectorizer.adapt(training_data[\"comments\"].values)\n",
        "vectorized_text = vectorizer(training_data[\"comments\"].values)\n",
        "\n",
        "dataset = Dataset.from_tensor_slices(\n",
        "    (vectorized_text, training_data[\"classifications\"])\n",
        ").cache().shuffle(160000).batch(16).prefetch(8)\n",
        "dataset_length = len(dataset)\n",
        "\n",
        "partitioned_dataset = {\n",
        "    \"training\": dataset.take(int(dataset_length * 0.7)),\n",
        "    \"testing\": dataset.skip(int(dataset_length * 0.7)).take(int(dataset_length * 0.1)),\n",
        "    \"validation\": dataset.skip(int(dataset_length * 0.8)).take(int(dataset_length * 0.2))\n",
        "}"
      ],
      "metadata": {
        "id": "E-A5tDb5kkRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Model Generation"
      ],
      "metadata": {
        "id": "HM3Um0X1ri9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(MAX_TOKENS + 1, 32))\n",
        "model.add(Bidirectional(LSTM(32, activation=\"tanh\")))\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dense(256, activation=\"relu\"))\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dense(6, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"BinaryCrossentropy\", optimizer=\"Adam\")"
      ],
      "metadata": {
        "id": "2Rhpfh04rmXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "emG1mwvWkGrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    partitioned_dataset[\"training\"],\n",
        "    epochs=10,\n",
        "    validation_data=partitioned_dataset[\"validation\"],\n",
        "    verbose=None\n",
        ")"
      ],
      "metadata": {
        "id": "ZbiTOiAXkKTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of Epochs"
      ],
      "metadata": {
        "id": "HgZnxcUe2NQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "font_manager.fontManager.addfont(DATASET_LOCATION + \"FreeMonoBold.ttf\")\n",
        "rcParams[\"font.family\"] = \"FreeMono\"\n",
        "\n",
        "pyplot.figure(figsize=(10, 10))\n",
        "pandas.DataFrame(history.history).plot()\n",
        "pyplot.title(\"Loss Metrics\")\n",
        "pyplot.xlabel(\"Epochs\")\n",
        "pyplot.ylabel(\"Loss\")\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "XXFHZWsm2SJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Model"
      ],
      "metadata": {
        "id": "oNsmNzIn8xBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_handle = {\n",
        "    \"precision\": Precision(),\n",
        "    \"recall\": Recall(),\n",
        "    \"accuracy\": CategoricalAccuracy()\n",
        "}\n",
        "\n",
        "for batch in partitioned_dataset[\"testing\"].as_numpy_iterator():\n",
        "    x, y = batch\n",
        "    y_res = model.predict(x, verbose=None).flatten()\n",
        "    y = y.flatten()\n",
        "\n",
        "    for handle in metrics_handle:\n",
        "        metrics_handle[handle].update_state(y, y_res)\n",
        "\n",
        "print(\"\\033[30;01mMetrics\\033[00m\")\n",
        "for handle in metrics_handle:\n",
        "    print(f\"  {handle}: {metrics_handle[handle].result().numpy()}\")"
      ],
      "metadata": {
        "id": "pO8pnIkK8zrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export the Model"
      ],
      "metadata": {
        "id": "6UaHv7Na9Hvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(DATASET_LOCATION + \"sentiment_analyser.h5\")"
      ],
      "metadata": {
        "id": "IJV-dd6e9Jn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Text"
      ],
      "metadata": {
        "id": "q_ItNaX36WfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = vectorizer(\"I will kill you!\")"
      ],
      "metadata": {
        "id": "OA_0izZRFq0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From Live Model"
      ],
      "metadata": {
        "id": "H0kvomX4Frbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.predict(expand_dims(input_text, 0), verbose=None)"
      ],
      "metadata": {
        "id": "VBZZ81Rn6Y8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From Saved Model"
      ],
      "metadata": {
        "id": "SnbznKxPFf5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(DATASET_LOCATION + \"sentiment_analyser.h5\")\n",
        "result = model.predict(expand_dims(input_text, 0), verbose=None)"
      ],
      "metadata": {
        "id": "PncprWw2Fhgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print Results"
      ],
      "metadata": {
        "id": "_AKe7GD6Fydj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\033[30;01mPrediction Results\\033[00m\")\n",
        "print(\"  toxic:\", result[0][0])\n",
        "print(\"  severe_toxic:\", result[0][1])\n",
        "print(\"  obscene:\", result[0][2])\n",
        "print(\"  threat:\", result[0][3])\n",
        "print(\"  insult:\", result[0][4])\n",
        "print(\"  identity_hate:\", result[0][5])"
      ],
      "metadata": {
        "id": "PSTclluYF07O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}